{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K40c (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 5004)\n",
      "/usr/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import lasagne\n",
    "from copy import deepcopy\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import random\n",
    "from skimage import exposure\n",
    "from skimage.morphology import binary_closing\n",
    "\n",
    "import sklearn.cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_patches(mri, label, half_patch_size=5, negative_subsample_ratio=300):\n",
    "    \n",
    "    assert mri.shape == label.shape\n",
    "    \n",
    "    z_size, x_size, y_size = mri.shape\n",
    "    positive_patches = []\n",
    "    negative_patches = []\n",
    "    \n",
    "    if np.sum(label) <= 0.5:\n",
    "        return ([], [])\n",
    "\n",
    "    for z in xrange(half_patch_size, z_size-half_patch_size):\n",
    "        if label[z,:,:].sum() > 1000: continue # workaround to remove white slices into the label dataset\n",
    "        for x in xrange(half_patch_size, x_size-half_patch_size):\n",
    "            for y in xrange(half_patch_size, y_size-half_patch_size):\n",
    "                if label[z,x,y] == 1:\n",
    "                    positive_patches.append(mri[z-half_patch_size:z+half_patch_size+1,x-half_patch_size:x+half_patch_size+1,y-half_patch_size:y+half_patch_size+1])\n",
    "                elif label[z,x,y] == 0:\n",
    "                    negative_patches.append(mri[z-half_patch_size:z+half_patch_size+1,x-half_patch_size:x+half_patch_size+1,y-half_patch_size:y+half_patch_size+1])\n",
    "    \n",
    "    random.shuffle(negative_patches)\n",
    "    number_of_negative_cases = int(len(negative_patches) / float(negative_subsample_ratio))\n",
    "    selected_negative_patches = deepcopy(negative_patches[:number_of_negative_cases])\n",
    "    del(negative_patches)\n",
    "    \n",
    "    return (positive_patches, selected_negative_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def patchimg2differentview(patch):\n",
    "    \n",
    "    z_size, x_size, y_size = patch.shape\n",
    "    \n",
    "    X = np.zeros((z_size * 3, x_size, y_size), dtype=np.float32)\n",
    "    \n",
    "    counter = 0\n",
    "    for z in xrange(z_size):\n",
    "        X[counter,:,:] = patch[z,:,:]\n",
    "        counter += 1\n",
    "    for x in xrange(x_size):\n",
    "        X[counter,:,:] = patch[:,x,:]\n",
    "        counter += 1\n",
    "    for y in xrange(y_size):\n",
    "        X[counter,:,:] = patch[:,:,y]\n",
    "        counter += 1\n",
    "    \n",
    "    return X.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def patches2CNNformat(patches, label, half_patch_size=5):\n",
    "    \n",
    "    X = np.zeros((len(patches), (2 * half_patch_size)+1, (half_patch_size*2)+1, (half_patch_size*2)+1), dtype=np.float32)\n",
    "    y = np.zeros((len(patches)), dtype=np.int32) * -1\n",
    "    \n",
    "    for i, patch in enumerate(patches):\n",
    "        #X[i,:,:,:] = patchimg2differentview(patch)\n",
    "        X[i,:,:,:] = patch\n",
    "        y[i] = label\n",
    "    \n",
    "    assert -1 not in y\n",
    "    \n",
    "    return X.astype(np.float32), y.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_resized_img(img, data_type = sitk.sitkFloat32):\n",
    "    \n",
    "    size = img.GetSize()\n",
    "    ratio = [1.0/i for i in img.GetSpacing()]\n",
    "    new_size = [int(size[i]/ratio[i]) for i in range(3)]\n",
    "    \n",
    "    rimage = sitk.Image(new_size, data_type)\n",
    "    rimage.SetSpacing((1,1,1))\n",
    "    rimage.SetOrigin(img.GetOrigin())\n",
    "    tx = sitk.Transform()\n",
    "    \n",
    "    interp = sitk.sitkLinear\n",
    "    if data_type == sitk.sitkInt16:\n",
    "        interp = sitk.sitkNearestNeighbor\n",
    "    \n",
    "    new_image = sitk.Resample(img, rimage, tx, interp, data_type)\n",
    "    \n",
    "    return sitk.GetArrayFromImage(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def needles2tips(only_needles, image_array, number_of_slices=3):\n",
    "    needles_masks_array = np.zeros_like(image_array).astype(float)  \n",
    "    for file_item in only_needles:\n",
    "        this_mask = file_item.astype(np.float)\n",
    "        if np.sum(this_mask) < (np.shape(this_mask)[0] * np.shape(this_mask)[1] * np.shape(this_mask)[2]):\n",
    "            this_mask = binary_closing(this_mask,selem=np.ones((3,3,3)))\n",
    "            found=False\n",
    "            row = np.shape(this_mask)[0]-1\n",
    "            while found==False and row > 0: #< np.shape(this_mask)[0]-1 :\n",
    "                #print(row)\n",
    "                this_row = this_mask[row,:,:]\n",
    "                if np.sum(this_row) > 0:\n",
    "                    #print(row)\n",
    "                    found = True\n",
    "                    temp = np.add(needles_masks_array[row:row+1+number_of_slices,:,:],this_mask[row:row+1+number_of_slices,:,:])\n",
    "                    temp[temp!=0] = 1\n",
    "                    needles_masks_array[row:row+1+number_of_slices,:,:] = temp \n",
    "                row -= 1\n",
    "    return needles_masks_array.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def needles2tips_PAOLO(needles, mri, number_of_slices=3):\n",
    "    tips = np.zeros_like(mri).astype(np.int32)\n",
    "    #print(tips.shape)\n",
    "    for needle in needles:\n",
    "        needle = needle.astype(np.int32)\n",
    "        #print(\"MIN %f, MAX %f\" % (needle.min(), needle.max()))\n",
    "        if np.sum(needle) < (np.shape(needle)[0] * np.shape(needle)[1] * np.shape(needle)[2]):\n",
    "            #print(\"Valid needle\")\n",
    "            needle = binary_closing(needle, selem=np.ones((3,3,3)))\n",
    "            needle[needle!=0]=1\n",
    "            #print(\" after closing: MIN %f, MAX %f \" % (needle.min(), needle.max()))\n",
    "            for z in range(np.shape(mri)[0]-1, 0, -1):\n",
    "                if 200 > np.sum(needle[z,:,:]) > 0.5 and z-number_of_slices-1 >= 0:\n",
    "                    #print(\" valid slice %d\" % z)\n",
    "                    tmp = deepcopy(needle)\n",
    "                    tmp[:z-number_of_slices-1,:,:] = 0\n",
    "                    tips[tmp!=0] = 1\n",
    "                    del(tmp)\n",
    "                    break\n",
    "    \n",
    "    tips[tips!=0]=1\n",
    "        \n",
    "    return tips.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_needles(needles, mri):\n",
    "    all_needles = np.zeros_like(mri).astype(np.int32)\n",
    "    \n",
    "    for needle in needles:\n",
    "        needle = needle.astype(np.int32)\n",
    "        needle[needle!=0]=1\n",
    "        \n",
    "        if np.sum(needle) < (needle.shape[0] * needle.shape[1] * needle.shape[2])/3.0:\n",
    "            all_needles[needle!=0] = 1\n",
    "    \n",
    "    all_needles[all_needles!=0]=1\n",
    "        \n",
    "    return all_needles.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_volume(img, half_patch_size=5):\n",
    "    npad = ((half_patch_size,half_patch_size),(half_patch_size,half_patch_size),(half_patch_size,half_patch_size))\n",
    "    return np.lib.pad(img, npad, \"constant\", constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_for_CNN(general_folder, half_patch_size=5):\n",
    "    folders_cases = os.listdir(general_folder)\n",
    "    \n",
    "    X_pos = np.ones((1, (2 * half_patch_size)+1, (half_patch_size*2)+1, (half_patch_size*2)+1), dtype=np.float32) * -1.0\n",
    "    y_pos = np.ones((1), dtype=np.int32) * -1.0\n",
    "    \n",
    "    X_neg = np.ones((1, (2 * half_patch_size)+1, (half_patch_size*2)+1, (half_patch_size*2)+1), dtype=np.float32) * -1.0\n",
    "    y_neg = np.ones((1), dtype=np.int32) * -1.0\n",
    "    \n",
    "    for folder_case in folders_cases:\n",
    "        print(\"Patient #%s\" % (folder_case))\n",
    "        full_case_path = general_folder + os.sep + folder_case\n",
    "        \n",
    "        volumetric_files = os.listdir(full_case_path)\n",
    "        \n",
    "        assert \"case.nrrd\" in volumetric_files\n",
    "        del(volumetric_files[volumetric_files.index(\"case.nrrd\")])\n",
    "        print(\" %d needles file\" %  (len(volumetric_files)))\n",
    "\n",
    "        \n",
    "        mri_sitk = sitk.ReadImage(full_case_path + os.sep + \"case.nrrd\")\n",
    "        \n",
    "        needles = []\n",
    "        valid_needles = 0\n",
    "        for volumetric_file in volumetric_files:\n",
    "            if volumetric_file == \"labelmap.nrrd\":\n",
    "                pass\n",
    "            label_sitk = sitk.ReadImage(full_case_path + os.sep + volumetric_file)\n",
    "            label = sitk.GetArrayFromImage(label_sitk)\n",
    "            background = int(np.around(np.percentile(label, 75), decimals=0))\n",
    "            #print('Background %d' % background)\n",
    "            filtered_label = np.zeros_like(label, dtype=np.int32)\n",
    "            filtered_label[label!=background] = 1\n",
    "            #print(np.sum(filtered_label))\n",
    "            \n",
    "            #label[label!=0]=1\n",
    "            if np.sum(filtered_label)>0:\n",
    "                needles.append(filtered_label)\n",
    "                valid_needles += 1\n",
    "                \n",
    "        print(\" %d valid needles\" %  (valid_needles))\n",
    "        \n",
    "        tips = merge_needles(needles, sitk.GetArrayFromImage(mri_sitk))\n",
    "        #tips_sitk = sitk.GetImageFromArray(tips)\n",
    "        #tips_sitk.CopyInformation(mri_sitk)\n",
    "        #tips = get_resized_img(tips_sitk, sitk.sitkInt16)\n",
    "        tips = pad_volume(tips, half_patch_size=half_patch_size)\n",
    "        tips[tips!=0]=1\n",
    "        \n",
    "        for z in range(tips.shape[0]):\n",
    "            if np.sum(tips[z,:,:]) > 500:\n",
    "                tips[z,:,:] = np.zeros_like(tips[z,:,:], dtype=np.int32)\n",
    "        \n",
    "        tips[tips!=0]=1\n",
    "        tips = tips.astype(np.int32)\n",
    "        #assert np.sum(tips) > 0.5\n",
    "        \n",
    "        mri = sitk.GetArrayFromImage(mri_sitk).astype(np.float32)\n",
    "        mri = pad_volume(mri, half_patch_size=half_patch_size)\n",
    "        \n",
    "        # DEBUG\n",
    "        #data_tag = str(datetime.datetime.now()).replace(\" \", \"_\")\n",
    "        #mri_sitk_DEBUG = sitk.GetImageFromArray(mri)\n",
    "        #tips_sitk_DEBUG = sitk.GetImageFromArray(tips)\n",
    "        #tips_sitk_DEBUG.CopyInformation(mri_sitk_DEBUG)\n",
    "        #sitk.WriteImage(mri_sitk_DEBUG, \"../DEBUG/mri_%s.nrrd\" % data_tag)\n",
    "        #sitk.WriteImage(tips_sitk_DEBUG, \"../DEBUG/tips_%s.nrrd\" % data_tag)\n",
    "        \n",
    "        positive_patches, negative_patches = extract_patches(mri, tips, half_patch_size=half_patch_size)\n",
    "        if len(positive_patches) > 0:\n",
    "            print(\" %d positive patches and %d negative patches\" % (len(positive_patches), len(negative_patches)))\n",
    "            X_temp_pos, y_temp_pos = patches2CNNformat(positive_patches, 1, half_patch_size=half_patch_size)\n",
    "            X_temp_neg, y_temp_neg = patches2CNNformat(negative_patches, 0, half_patch_size=half_patch_size)\n",
    "            \n",
    "            X_pos = np.concatenate((X_pos, X_temp_pos), axis=0)\n",
    "            y_pos = np.concatenate((y_pos, y_temp_pos), axis=0)\n",
    "            X_neg = np.concatenate((X_neg, X_temp_neg), axis=0)\n",
    "            y_neg = np.concatenate((y_neg, y_temp_neg), axis=0)\n",
    "    \n",
    "    assert -1 in X_pos[0,:,:,:] and y_pos[0] == -1\n",
    "    X_pos = X_pos[1:,:,:,:].astype(np.float32)\n",
    "    y_pos = y_pos[1:].astype(np.int32)\n",
    "    \n",
    "    assert -1 in X_neg[0,:,:,:] and y_neg[0] == -1\n",
    "    X_neg = X_neg[1:,:,:,:].astype(np.float32)\n",
    "    y_neg = y_neg[1:].astype(np.int32)\n",
    "    \n",
    "    return X_pos, y_pos, X_neg, y_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_Xy_posneg(X_pos, y_pos, X_neg, y_neg, test_size1=0.20, test_size2=0.33):\n",
    "    \n",
    "    X_pos_train, X_pos_test, y_pos_train, y_pos_test = sklearn.cross_validation.train_test_split(X_pos, y_pos, test_size=test_size1)\n",
    "    X_pos_test, X_pos_val, y_pos_test, y_pos_val = sklearn.cross_validation.train_test_split(X_pos_test, y_pos_test, test_size=test_size2)\n",
    "\n",
    "    X_neg_train, X_neg_test, y_neg_train, y_neg_test = sklearn.cross_validation.train_test_split(X_neg, y_neg, test_size=test_size1)\n",
    "    X_neg_test, X_neg_val, y_neg_test, y_neg_val = sklearn.cross_validation.train_test_split(X_neg_test, y_neg_test, test_size=test_size2)\n",
    "\n",
    "    X_train = np.concatenate((X_neg_train, X_pos_train), axis=0).astype(np.float32)\n",
    "    y_train = np.concatenate((y_neg_train, y_pos_train), axis=0).astype(np.int32)\n",
    "\n",
    "    X_val = np.concatenate((X_neg_val, X_pos_val), axis=0).astype(np.float32)\n",
    "    y_val = np.concatenate((y_neg_val, y_pos_val), axis=0).astype(np.int32)\n",
    "\n",
    "    X_test = np.concatenate((X_neg_test, X_pos_test), axis=0).astype(np.float32)\n",
    "    y_test = np.concatenate((y_neg_test, y_pos_test), axis=0).astype(np.int32)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "half_patch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient #072\n",
      " 21 needles file\n",
      " 21 valid needles\n",
      " 17506 positive patches and 37381 negative patches\n",
      "Patient #008\n",
      " 9 needles file\n",
      " 9 valid needles\n",
      " 3204 positive patches and 10389 negative patches\n",
      "Patient #071\n",
      " 14 needles file\n",
      " 14 valid needles\n",
      " 10650 positive patches and 44814 negative patches\n",
      "Patient #076\n",
      " 13 needles file\n",
      " 12 valid needles\n",
      " 11322 positive patches and 34867 negative patches\n",
      "Patient #022\n",
      " 8 needles file\n",
      " 8 valid needles\n",
      " 7383 positive patches and 38695 negative patches\n",
      "Patient #012\n",
      " 10 needles file\n",
      " 10 valid needles\n",
      " 4112 positive patches and 11852 negative patches\n",
      "Patient #069\n",
      " 36 needles file\n",
      " 36 valid needles\n",
      " 29573 positive patches and 34806 negative patches\n",
      "Patient #056\n",
      " 13 needles file\n",
      " 13 valid needles\n",
      " 7642 positive patches and 40772 negative patches\n",
      "Patient #007\n",
      " 8 needles file\n",
      " 8 valid needles\n",
      " 4952 positive patches and 20463 negative patches\n",
      "Patient #053\n",
      " 9 needles file\n",
      " 9 valid needles\n",
      " 7905 positive patches and 32885 negative patches\n",
      "Patient #041\n",
      " 33 needles file\n",
      " 33 valid needles\n",
      " 28091 positive patches and 34811 negative patches\n",
      "Patient #035\n",
      " 18 needles file\n",
      " 18 valid needles\n",
      " 15728 positive patches and 37387 negative patches\n",
      "Patient #026\n",
      " 15 needles file\n",
      " 15 valid needles\n",
      " 13879 positive patches and 44803 negative patches\n",
      "Patient #048\n",
      " 20 needles file\n",
      " 20 valid needles\n",
      " 17741 positive patches and 44790 negative patches\n",
      "Patient #039\n",
      " 16 needles file\n",
      " 16 valid needles\n",
      " 10864 positive patches and 29798 negative patches\n",
      "Patient #024\n",
      " 8 needles file\n",
      " 8 valid needles\n",
      " 3477 positive patches and 10388 negative patches\n",
      "Patient #025\n",
      " 10 needles file\n",
      " 10 valid needles\n",
      " 5222 positive patches and 29817 negative patches\n",
      "Patient #033\n",
      " 6 needles file\n",
      " 6 valid needles\n",
      " 4448 positive patches and 37425 negative patches\n",
      "Patient #027\n",
      " 6 needles file\n",
      " 6 valid needles\n",
      " 1171 positive patches and 14396 negative patches\n",
      "Patient #021\n",
      " 7 needles file\n",
      " 7 valid needles\n",
      " 1527 positive patches and 10261 negative patches\n",
      "Patient #064\n",
      " 20 needles file\n",
      " 20 valid needles\n",
      " 18387 positive patches and 37378 negative patches\n",
      "Patient #059\n",
      " 36 needles file\n",
      " 34 valid needles\n",
      " 32898 positive patches and 44740 negative patches\n",
      "Patient #063\n",
      " 27 needles file\n",
      " 27 valid needles\n",
      " 24301 positive patches and 44768 negative patches\n",
      "Patient #060\n",
      " 12 needles file\n",
      " 11 valid needles\n",
      " 7601 positive patches and 29809 negative patches\n",
      "Patient #070\n",
      " 16 needles file\n",
      " 16 valid needles\n",
      " 11873 positive patches and 44810 negative patches\n",
      "Patient #034\n",
      " 8 needles file\n",
      " 8 valid needles\n",
      " 3712 positive patches and 13786 negative patches\n",
      "Patient #074\n",
      " 17 needles file\n",
      " 15 valid needles\n",
      " 11551 positive patches and 39741 negative patches\n",
      "Patient #029\n",
      " 9 needles file\n",
      " 9 valid needles\n",
      " 3960 positive patches and 32898 negative patches\n",
      "Patient #031\n",
      " 17 needles file\n",
      " 16 valid needles\n",
      " 15801 positive patches and 49672 negative patches\n",
      "Patient #044\n",
      " 15 needles file\n",
      " 15 valid needles\n",
      " 10493 positive patches and 32877 negative patches\n",
      "Patient #014\n",
      " 9 needles file\n",
      " 9 valid needles\n",
      " 2107 positive patches and 13791 negative patches\n",
      "Patient #023\n",
      " 15 needles file\n",
      " 15 valid needles\n",
      " 7913 positive patches and 31973 negative patches\n",
      "Patient #075\n",
      " 8 needles file\n",
      " 8 valid needles\n",
      " 5760 positive patches and 34885 negative patches\n",
      "Patient #077\n",
      " 8 needles file\n",
      " 8 valid needles\n",
      " 3898 positive patches and 32357 negative patches\n",
      "Patient #037\n",
      " 7 needles file\n",
      " 7 valid needles\n",
      " 4127 positive patches and 29821 negative patches\n",
      "Patient #030\n",
      " 16 needles file\n",
      " 16 valid needles\n",
      " 7878 positive patches and 34878 negative patches\n",
      "Patient #040\n",
      " 15 needles file\n",
      " 15 valid needles\n",
      " 9122 positive patches and 30434 negative patches\n",
      "Patient #042\n",
      " 19 needles file\n",
      " 19 valid needles\n",
      " 7309 positive patches and 17537 negative patches\n",
      "Patient #058\n",
      " 12 needles file\n",
      " 12 valid needles\n",
      " 5440 positive patches and 37226 negative patches\n",
      "Patient #038\n",
      " 10 needles file\n",
      " 10 valid needles\n",
      " 7820 positive patches and 43391 negative patches\n",
      "Patient #050\n",
      " 21 needles file\n",
      " 21 valid needles\n",
      " 17851 positive patches and 34845 negative patches\n",
      "Patient #045\n",
      " 18 needles file\n",
      " 18 valid needles\n",
      " 8964 positive patches and 34875 negative patches\n",
      "Patient #016\n",
      " 16 needles file\n",
      " 16 valid needles\n",
      " 3866 positive patches and 17548 negative patches\n",
      "Patient #049\n",
      " 10 needles file\n",
      " 10 valid needles\n",
      " 8957 positive patches and 39750 negative patches\n"
     ]
    }
   ],
   "source": [
    "X_pos, y_pos, X_neg, y_neg = data_for_CNN(\"../LabelMaps\", half_patch_size=half_patch_size)\n",
    "#saved_data = np.load('../patches_jun27_halfpatchsize5odd.npz')\n",
    "#X, y = saved_data['arr_0'], saved_data['arr_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.savez('../patches_jul1_halfpatchsize5odd.npz', X_pos, y_pos, X_neg, y_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = join_Xy_posneg(X_pos, y_pos, X_neg, y_neg, test_size1=0.10, test_size2=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m, s = X_train.mean(), X_train.std()\n",
    "\n",
    "X_train -= m\n",
    "X_train /= s\n",
    "\n",
    "X_val -= m\n",
    "X_val /= s\n",
    "\n",
    "X_test -= m\n",
    "X_test /= s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('meanandstd_jan12.npz', m=m, s=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1667318, 17, 17, 17), (124122, 17, 17, 17), (61136, 17, 17, 17))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=True):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn(single_entry_shape, input_var=None):\n",
    "    \n",
    "    network = lasagne.layers.InputLayer(shape=(None, single_entry_shape[0], single_entry_shape[1], \n",
    "                                               single_entry_shape[2]),\n",
    "                                        input_var=input_var)\n",
    "    \n",
    "    network = lasagne.layers.batch_norm(lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=480, filter_size=(3, 3),  #120\n",
    "            nonlinearity=lasagne.nonlinearities.leaky_rectify,\n",
    "            W=lasagne.init.HeNormal()))\n",
    "    #network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "    \n",
    "    network = lasagne.layers.batch_norm(lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=240, filter_size=(2, 2), #120\n",
    "            nonlinearity=lasagne.nonlinearities.leaky_rectify))\n",
    "    \n",
    "    network = lasagne.layers.DenseLayer(\n",
    "           lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=120, #120*2*2\n",
    "           nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "    \n",
    "    network = lasagne.layers.DenseLayer(\n",
    "           lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=60,\n",
    "           nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "    \n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=2,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 80\n",
    "batchsize = 256\n",
    "single_entry_shape = X_train.shape[1:]\n",
    "\n",
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "network = build_cnn(single_entry_shape, input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1 started on 2017-01-13 04:14\n",
      "Epoch 1 of 80 took 1223.758s\n",
      "  training loss:\t\t0.135461\n",
      "  validation loss:\t\t0.065935\n",
      "  validation accuracy:\t\t97.59 %\n",
      "Epoch 2 started on 2017-01-13 04:34\n",
      "Epoch 2 of 80 took 1217.295s\n",
      "  training loss:\t\t0.077726\n",
      "  validation loss:\t\t0.047359\n",
      "  validation accuracy:\t\t98.36 %\n",
      "Epoch 3 started on 2017-01-13 04:54\n",
      "Epoch 3 of 80 took 1217.519s\n",
      "  training loss:\t\t0.060621\n",
      "  validation loss:\t\t0.040408\n",
      "  validation accuracy:\t\t98.57 %\n",
      "Epoch 4 started on 2017-01-13 05:15\n",
      "Epoch 4 of 80 took 1217.471s\n",
      "  training loss:\t\t0.051880\n",
      "  validation loss:\t\t0.035199\n",
      "  validation accuracy:\t\t98.83 %\n",
      "Epoch 5 started on 2017-01-13 05:35\n",
      "Epoch 5 of 80 took 1217.400s\n",
      "  training loss:\t\t0.046052\n",
      "  validation loss:\t\t0.029321\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 6 started on 2017-01-13 05:55\n",
      "Epoch 6 of 80 took 1217.564s\n",
      "  training loss:\t\t0.042363\n",
      "  validation loss:\t\t0.029036\n",
      "  validation accuracy:\t\t99.00 %\n",
      "Epoch 7 started on 2017-01-13 06:15\n",
      "Epoch 7 of 80 took 1217.497s\n",
      "  training loss:\t\t0.039168\n",
      "  validation loss:\t\t0.029583\n",
      "  validation accuracy:\t\t98.98 %\n",
      "Epoch 8 started on 2017-01-13 06:36\n",
      "Epoch 8 of 80 took 1217.604s\n",
      "  training loss:\t\t0.036559\n",
      "  validation loss:\t\t0.024656\n",
      "  validation accuracy:\t\t99.21 %\n",
      "Epoch 9 started on 2017-01-13 06:56\n",
      "Epoch 9 of 80 took 1217.450s\n",
      "  training loss:\t\t0.034614\n",
      "  validation loss:\t\t0.026803\n",
      "  validation accuracy:\t\t99.09 %\n",
      "Epoch 10 started on 2017-01-13 07:16\n",
      "Epoch 10 of 80 took 1217.609s\n",
      "  training loss:\t\t0.032837\n",
      "  validation loss:\t\t0.022233\n",
      "  validation accuracy:\t\t99.31 %\n",
      "Epoch 11 started on 2017-01-13 07:37\n",
      "Epoch 11 of 80 took 1217.452s\n",
      "  training loss:\t\t0.031657\n",
      "  validation loss:\t\t0.024794\n",
      "  validation accuracy:\t\t99.16 %\n",
      "Epoch 12 started on 2017-01-13 07:57\n",
      "Epoch 12 of 80 took 1217.460s\n",
      "  training loss:\t\t0.030036\n",
      "  validation loss:\t\t0.021837\n",
      "  validation accuracy:\t\t99.31 %\n",
      "Epoch 13 started on 2017-01-13 08:17\n",
      "Epoch 13 of 80 took 1217.477s\n",
      "  training loss:\t\t0.028944\n",
      "  validation loss:\t\t0.022012\n",
      "  validation accuracy:\t\t99.32 %\n",
      "Epoch 14 started on 2017-01-13 08:37\n",
      "Epoch 14 of 80 took 1217.483s\n",
      "  training loss:\t\t0.028082\n",
      "  validation loss:\t\t0.020553\n",
      "  validation accuracy:\t\t99.36 %\n",
      "Epoch 15 started on 2017-01-13 08:58\n",
      "Epoch 15 of 80 took 1217.444s\n",
      "  training loss:\t\t0.027151\n",
      "  validation loss:\t\t0.019846\n",
      "  validation accuracy:\t\t99.40 %\n",
      "Epoch 16 started on 2017-01-13 09:18\n",
      "Epoch 16 of 80 took 1217.298s\n",
      "  training loss:\t\t0.026005\n",
      "  validation loss:\t\t0.020298\n",
      "  validation accuracy:\t\t99.35 %\n",
      "Epoch 17 started on 2017-01-13 09:38\n",
      "Epoch 17 of 80 took 1217.388s\n",
      "  training loss:\t\t0.025287\n",
      "  validation loss:\t\t0.020115\n",
      "  validation accuracy:\t\t99.35 %\n",
      "Epoch 18 started on 2017-01-13 09:59\n",
      "Epoch 18 of 80 took 1217.341s\n",
      "  training loss:\t\t0.024556\n",
      "  validation loss:\t\t0.019392\n",
      "  validation accuracy:\t\t99.40 %\n",
      "Epoch 19 started on 2017-01-13 10:19\n",
      "Epoch 19 of 80 took 1217.367s\n",
      "  training loss:\t\t0.023758\n",
      "  validation loss:\t\t0.017573\n",
      "  validation accuracy:\t\t99.49 %\n",
      "Epoch 20 started on 2017-01-13 10:39\n",
      "Epoch 20 of 80 took 1217.355s\n",
      "  training loss:\t\t0.023065\n",
      "  validation loss:\t\t0.018449\n",
      "  validation accuracy:\t\t99.43 %\n",
      "Epoch 21 started on 2017-01-13 10:59\n",
      "Epoch 21 of 80 took 1217.329s\n",
      "  training loss:\t\t0.022831\n",
      "  validation loss:\t\t0.017923\n",
      "  validation accuracy:\t\t99.44 %\n",
      "Epoch 22 started on 2017-01-13 11:20\n",
      "Epoch 22 of 80 took 1217.313s\n",
      "  training loss:\t\t0.021770\n",
      "  validation loss:\t\t0.018018\n",
      "  validation accuracy:\t\t99.45 %\n",
      "Epoch 23 started on 2017-01-13 11:40\n",
      "Epoch 23 of 80 took 1217.396s\n",
      "  training loss:\t\t0.021267\n",
      "  validation loss:\t\t0.015803\n",
      "  validation accuracy:\t\t99.53 %\n",
      "Epoch 24 started on 2017-01-13 12:00\n",
      "Epoch 24 of 80 took 1217.334s\n",
      "  training loss:\t\t0.020583\n",
      "  validation loss:\t\t0.017522\n",
      "  validation accuracy:\t\t99.44 %\n",
      "Epoch 25 started on 2017-01-13 12:21\n",
      "Epoch 25 of 80 took 1217.285s\n",
      "  training loss:\t\t0.020537\n",
      "  validation loss:\t\t0.016150\n",
      "  validation accuracy:\t\t99.52 %\n",
      "Epoch 26 started on 2017-01-13 12:41\n",
      "Epoch 26 of 80 took 1217.167s\n",
      "  training loss:\t\t0.020145\n",
      "  validation loss:\t\t0.015877\n",
      "  validation accuracy:\t\t99.54 %\n",
      "Epoch 27 started on 2017-01-13 13:01\n",
      "Epoch 27 of 80 took 1217.170s\n",
      "  training loss:\t\t0.019510\n",
      "  validation loss:\t\t0.015327\n",
      "  validation accuracy:\t\t99.54 %\n",
      "Epoch 28 started on 2017-01-13 13:22\n",
      "Epoch 28 of 80 took 1217.124s\n",
      "  training loss:\t\t0.019093\n",
      "  validation loss:\t\t0.015758\n",
      "  validation accuracy:\t\t99.52 %\n",
      "Epoch 29 started on 2017-01-13 13:42\n",
      "Epoch 29 of 80 took 1217.218s\n",
      "  training loss:\t\t0.018667\n",
      "  validation loss:\t\t0.015195\n",
      "  validation accuracy:\t\t99.55 %\n",
      "Epoch 30 started on 2017-01-13 14:02\n",
      "Epoch 30 of 80 took 1217.195s\n",
      "  training loss:\t\t0.018352\n",
      "  validation loss:\t\t0.015292\n",
      "  validation accuracy:\t\t99.55 %\n",
      "Epoch 31 started on 2017-01-13 14:22\n",
      "Epoch 31 of 80 took 1217.307s\n",
      "  training loss:\t\t0.018037\n",
      "  validation loss:\t\t0.015868\n",
      "  validation accuracy:\t\t99.52 %\n",
      "Epoch 32 started on 2017-01-13 14:43\n",
      "Epoch 32 of 80 took 1217.289s\n",
      "  training loss:\t\t0.017604\n",
      "  validation loss:\t\t0.015361\n",
      "  validation accuracy:\t\t99.54 %\n",
      "Epoch 33 started on 2017-01-13 15:03\n",
      "Epoch 33 of 80 took 1217.257s\n",
      "  training loss:\t\t0.017134\n",
      "  validation loss:\t\t0.015327\n",
      "  validation accuracy:\t\t99.57 %\n",
      "Epoch 34 started on 2017-01-13 15:23\n",
      "Epoch 34 of 80 took 1217.265s\n",
      "  training loss:\t\t0.016902\n",
      "  validation loss:\t\t0.014382\n",
      "  validation accuracy:\t\t99.58 %\n",
      "Epoch 35 started on 2017-01-13 15:44\n",
      "Epoch 35 of 80 took 1217.318s\n",
      "  training loss:\t\t0.016889\n",
      "  validation loss:\t\t0.014604\n",
      "  validation accuracy:\t\t99.58 %\n",
      "Epoch 36 started on 2017-01-13 16:04\n",
      "Epoch 36 of 80 took 1217.168s\n",
      "  training loss:\t\t0.016444\n",
      "  validation loss:\t\t0.012857\n",
      "  validation accuracy:\t\t99.62 %\n",
      "Epoch 37 started on 2017-01-13 16:24\n",
      "Epoch 37 of 80 took 1217.273s\n",
      "  training loss:\t\t0.016119\n",
      "  validation loss:\t\t0.014930\n",
      "  validation accuracy:\t\t99.57 %\n",
      "Epoch 38 started on 2017-01-13 16:44\n",
      "Epoch 38 of 80 took 1217.134s\n",
      "  training loss:\t\t0.015883\n",
      "  validation loss:\t\t0.017145\n",
      "  validation accuracy:\t\t99.47 %\n",
      "Epoch 39 started on 2017-01-13 17:05\n",
      "Epoch 39 of 80 took 1217.058s\n",
      "  training loss:\t\t0.015569\n",
      "  validation loss:\t\t0.016330\n",
      "  validation accuracy:\t\t99.53 %\n",
      "Epoch 40 started on 2017-01-13 17:25\n",
      "Epoch 40 of 80 took 1216.978s\n",
      "  training loss:\t\t0.015343\n",
      "  validation loss:\t\t0.013693\n",
      "  validation accuracy:\t\t99.61 %\n",
      "Epoch 41 started on 2017-01-13 17:45\n",
      "Epoch 41 of 80 took 1217.135s\n",
      "  training loss:\t\t0.015150\n",
      "  validation loss:\t\t0.014040\n",
      "  validation accuracy:\t\t99.58 %\n",
      "Epoch 42 started on 2017-01-13 18:06\n",
      "Epoch 42 of 80 took 1217.239s\n",
      "  training loss:\t\t0.014995\n",
      "  validation loss:\t\t0.015517\n",
      "  validation accuracy:\t\t99.56 %\n",
      "Epoch 43 started on 2017-01-13 18:26\n",
      "Epoch 43 of 80 took 1217.273s\n",
      "  training loss:\t\t0.014712\n",
      "  validation loss:\t\t0.013916\n",
      "  validation accuracy:\t\t99.60 %\n",
      "Epoch 44 started on 2017-01-13 18:46\n",
      "Epoch 44 of 80 took 1217.226s\n",
      "  training loss:\t\t0.014474\n",
      "  validation loss:\t\t0.012634\n",
      "  validation accuracy:\t\t99.63 %\n",
      "Epoch 45 started on 2017-01-13 19:06\n",
      "Epoch 45 of 80 took 1217.330s\n",
      "  training loss:\t\t0.014172\n",
      "  validation loss:\t\t0.014079\n",
      "  validation accuracy:\t\t99.60 %\n",
      "Epoch 46 started on 2017-01-13 19:27\n",
      "Epoch 46 of 80 took 1217.335s\n",
      "  training loss:\t\t0.014317\n",
      "  validation loss:\t\t0.013549\n",
      "  validation accuracy:\t\t99.62 %\n",
      "Epoch 47 started on 2017-01-13 19:47\n",
      "Epoch 47 of 80 took 1217.170s\n",
      "  training loss:\t\t0.013560\n",
      "  validation loss:\t\t0.013239\n",
      "  validation accuracy:\t\t99.62 %\n",
      "Epoch 48 started on 2017-01-13 20:07\n",
      "Epoch 48 of 80 took 1217.114s\n",
      "  training loss:\t\t0.014005\n",
      "  validation loss:\t\t0.013146\n",
      "  validation accuracy:\t\t99.63 %\n",
      "Epoch 49 started on 2017-01-13 20:28\n",
      "Epoch 49 of 80 took 1217.238s\n",
      "  training loss:\t\t0.013387\n",
      "  validation loss:\t\t0.014213\n",
      "  validation accuracy:\t\t99.60 %\n",
      "Epoch 50 started on 2017-01-13 20:48\n",
      "Epoch 50 of 80 took 1217.178s\n",
      "  training loss:\t\t0.013374\n",
      "  validation loss:\t\t0.013878\n",
      "  validation accuracy:\t\t99.60 %\n",
      "Epoch 51 started on 2017-01-13 21:08\n",
      "Epoch 51 of 80 took 1217.213s\n",
      "  training loss:\t\t0.013016\n",
      "  validation loss:\t\t0.015456\n",
      "  validation accuracy:\t\t99.55 %\n",
      "Epoch 52 started on 2017-01-13 21:28\n",
      "Epoch 52 of 80 took 1217.207s\n",
      "  training loss:\t\t0.012902\n",
      "  validation loss:\t\t0.013616\n",
      "  validation accuracy:\t\t99.60 %\n",
      "Epoch 53 started on 2017-01-13 21:49\n",
      "Epoch 53 of 80 took 1217.218s\n",
      "  training loss:\t\t0.012722\n",
      "  validation loss:\t\t0.012720\n",
      "  validation accuracy:\t\t99.64 %\n",
      "Epoch 54 started on 2017-01-13 22:09\n",
      "Epoch 54 of 80 took 1217.216s\n",
      "  training loss:\t\t0.012654\n",
      "  validation loss:\t\t0.014947\n",
      "  validation accuracy:\t\t99.59 %\n",
      "Epoch 55 started on 2017-01-13 22:29\n",
      "Epoch 55 of 80 took 1217.149s\n",
      "  training loss:\t\t0.012649\n",
      "  validation loss:\t\t0.012970\n",
      "  validation accuracy:\t\t99.65 %\n",
      "Epoch 56 started on 2017-01-13 22:50\n",
      "Epoch 56 of 80 took 1217.150s\n",
      "  training loss:\t\t0.012445\n",
      "  validation loss:\t\t0.014569\n",
      "  validation accuracy:\t\t99.60 %\n",
      "Epoch 57 started on 2017-01-13 23:10\n",
      "Epoch 57 of 80 took 1217.199s\n",
      "  training loss:\t\t0.012056\n",
      "  validation loss:\t\t0.012764\n",
      "  validation accuracy:\t\t99.65 %\n",
      "Epoch 58 started on 2017-01-13 23:30\n",
      "Epoch 58 of 80 took 1217.249s\n",
      "  training loss:\t\t0.012289\n",
      "  validation loss:\t\t0.013403\n",
      "  validation accuracy:\t\t99.62 %\n",
      "Epoch 59 started on 2017-01-13 23:50\n",
      "Epoch 59 of 80 took 1217.367s\n",
      "  training loss:\t\t0.011916\n",
      "  validation loss:\t\t0.012603\n",
      "  validation accuracy:\t\t99.65 %\n",
      "Epoch 60 started on 2017-01-14 00:11\n",
      "Epoch 60 of 80 took 1217.063s\n",
      "  training loss:\t\t0.011677\n",
      "  validation loss:\t\t0.011993\n",
      "  validation accuracy:\t\t99.67 %\n",
      "Epoch 61 started on 2017-01-14 00:31\n",
      "Epoch 61 of 80 took 1217.044s\n",
      "  training loss:\t\t0.011519\n",
      "  validation loss:\t\t0.013932\n",
      "  validation accuracy:\t\t99.61 %\n",
      "Epoch 62 started on 2017-01-14 00:51\n",
      "Epoch 62 of 80 took 1217.076s\n",
      "  training loss:\t\t0.011276\n",
      "  validation loss:\t\t0.013685\n",
      "  validation accuracy:\t\t99.64 %\n",
      "Epoch 63 started on 2017-01-14 01:12\n",
      "Epoch 63 of 80 took 1217.237s\n",
      "  training loss:\t\t0.011460\n",
      "  validation loss:\t\t0.013510\n",
      "  validation accuracy:\t\t99.65 %\n",
      "Epoch 64 started on 2017-01-14 01:32\n",
      "Epoch 64 of 80 took 1217.186s\n",
      "  training loss:\t\t0.011278\n",
      "  validation loss:\t\t0.013160\n",
      "  validation accuracy:\t\t99.62 %\n",
      "Epoch 65 started on 2017-01-14 01:52\n",
      "Epoch 65 of 80 took 1217.048s\n",
      "  training loss:\t\t0.011324\n",
      "  validation loss:\t\t0.012467\n",
      "  validation accuracy:\t\t99.66 %\n",
      "Epoch 66 started on 2017-01-14 02:12\n",
      "Epoch 66 of 80 took 1217.164s\n",
      "  training loss:\t\t0.010982\n",
      "  validation loss:\t\t0.013005\n",
      "  validation accuracy:\t\t99.65 %\n",
      "Epoch 67 started on 2017-01-14 02:33\n",
      "Epoch 67 of 80 took 1217.223s\n",
      "  training loss:\t\t0.010781\n",
      "  validation loss:\t\t0.012934\n",
      "  validation accuracy:\t\t99.66 %\n",
      "Epoch 68 started on 2017-01-14 02:53\n",
      "Epoch 68 of 80 took 1217.145s\n",
      "  training loss:\t\t0.010646\n",
      "  validation loss:\t\t0.012084\n",
      "  validation accuracy:\t\t99.68 %\n",
      "Epoch 69 started on 2017-01-14 03:13\n",
      "Epoch 69 of 80 took 1217.068s\n",
      "  training loss:\t\t0.010826\n",
      "  validation loss:\t\t0.013010\n",
      "  validation accuracy:\t\t99.66 %\n",
      "Epoch 70 started on 2017-01-14 03:34\n",
      "Epoch 70 of 80 took 1217.098s\n",
      "  training loss:\t\t0.010497\n",
      "  validation loss:\t\t0.012663\n",
      "  validation accuracy:\t\t99.65 %\n",
      "Epoch 71 started on 2017-01-14 03:54\n",
      "Epoch 71 of 80 took 1216.976s\n",
      "  training loss:\t\t0.010371\n",
      "  validation loss:\t\t0.013450\n",
      "  validation accuracy:\t\t99.65 %\n",
      "Epoch 72 started on 2017-01-14 04:14\n",
      "Epoch 72 of 80 took 1217.093s\n",
      "  training loss:\t\t0.010418\n",
      "  validation loss:\t\t0.012772\n",
      "  validation accuracy:\t\t99.65 %\n",
      "Epoch 73 started on 2017-01-14 04:34\n",
      "Epoch 73 of 80 took 1217.081s\n",
      "  training loss:\t\t0.010200\n",
      "  validation loss:\t\t0.012790\n",
      "  validation accuracy:\t\t99.66 %\n",
      "Epoch 74 started on 2017-01-14 04:55\n",
      "Epoch 74 of 80 took 1217.123s\n",
      "  training loss:\t\t0.010156\n",
      "  validation loss:\t\t0.013168\n",
      "  validation accuracy:\t\t99.65 %\n",
      "Epoch 75 started on 2017-01-14 05:15\n",
      "Epoch 75 of 80 took 1217.083s\n",
      "  training loss:\t\t0.010123\n",
      "  validation loss:\t\t0.012506\n",
      "  validation accuracy:\t\t99.68 %\n",
      "Epoch 76 started on 2017-01-14 05:35\n",
      "Epoch 76 of 80 took 1217.076s\n",
      "  training loss:\t\t0.009782\n",
      "  validation loss:\t\t0.012404\n",
      "  validation accuracy:\t\t99.68 %\n",
      "Epoch 77 started on 2017-01-14 05:56\n",
      "Epoch 77 of 80 took 1217.064s\n",
      "  training loss:\t\t0.009679\n",
      "  validation loss:\t\t0.012977\n",
      "  validation accuracy:\t\t99.67 %\n",
      "Epoch 78 started on 2017-01-14 06:16\n",
      "Epoch 78 of 80 took 1216.999s\n",
      "  training loss:\t\t0.009699\n",
      "  validation loss:\t\t0.013407\n",
      "  validation accuracy:\t\t99.66 %\n",
      "Epoch 79 started on 2017-01-14 06:36\n",
      "Epoch 79 of 80 took 1216.921s\n",
      "  training loss:\t\t0.009607\n",
      "  validation loss:\t\t0.011417\n",
      "  validation accuracy:\t\t99.72 %\n",
      "Epoch 80 started on 2017-01-14 06:56\n",
      "Epoch 80 of 80 took 1216.956s\n",
      "  training loss:\t\t0.009452\n",
      "  validation loss:\t\t0.012769\n",
      "  validation accuracy:\t\t99.68 %\n",
      "Final results:\n",
      "  test loss:\t\t\t0.011779\n",
      "  test accuracy:\t\t99.69 %\n"
     ]
    }
   ],
   "source": [
    "# Create a loss expression for training, i.e., a scalar objective we want\n",
    "# to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var) # multiclass_hinge_loss\n",
    "loss = loss.mean()\n",
    "# We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "# Create update expressions for training, i.e., how to modify the\n",
    "# parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "# Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=0.001, momentum=0.9)\n",
    "\n",
    "# Create a loss expression for validation/testing. The crucial difference\n",
    "# here is that we do a deterministic forward pass through the network,\n",
    "# disabling dropout layers.\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction, target_var)\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                    dtype=theano.config.floatX)\n",
    "\n",
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    print(\"Epoch %d started on %s\" % (epoch + 1, now.strftime(\"%Y-%m-%d %H:%M\")))\n",
    "    \n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, batchsize, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batchsize, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "\n",
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, batchsize, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SAVE NETWORK\n",
    "np.savez('model_needle_jan12.npz', *lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINED\n"
     ]
    }
   ],
   "source": [
    "print('TRAINED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LOAD NETWORK\n",
    "\"\"\"\n",
    "with np.load('../model_needle_july1.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "lasagne.layers.set_all_param_values(network, param_values)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "predict_fn = theano.function([input_var], T.argmax(test_prediction, axis=1))\n",
    "predict_confidence = theano.function([input_var], test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind = 17\n",
    "a = X_test[ind,:,:,:].reshape((1,(2*half_patch_size)+1,(2*half_patch_size)+1,(2*half_patch_size)+1))\n",
    "predict_label_patch = predict_fn(a)[0]\n",
    "\n",
    "print(predict_label_patch, y_test[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imshow(a[0,5,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 181, 156)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_pat_sitk = sitk.ReadImage('LabelMaps/64/case.nrrd')\n",
    "test_pat = sitk.GetArrayFromImage(sitk.ReadImage('../testing_pat/028/case.nrrd')).astype(np.float32)\n",
    "test_pat = test_pat[140:,15:180,80:230]\n",
    "test_pat = pad_volume(test_pat, half_patch_size=half_patch_size)\n",
    "\n",
    "pat_sitk = sitk.GetImageFromArray(test_pat.astype(np.float32))\n",
    "sitk.WriteImage(pat_sitk, '../test_pat.nrrd')\n",
    "\n",
    "final_label = np.zeros_like(test_pat)\n",
    "test_pat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "z_size, x_size, y_size = test_pat.shape\n",
    "\n",
    "for z in xrange(half_patch_size, z_size-half_patch_size):\n",
    "    print(z)\n",
    "    for x in xrange(half_patch_size, x_size-half_patch_size):\n",
    "        for y in xrange(half_patch_size, y_size-half_patch_size):\n",
    "            patient_patch_img = test_pat[z-half_patch_size:z+half_patch_size+1,x-half_patch_size:x+half_patch_size+1,y-half_patch_size:y+half_patch_size+1]\n",
    "            #patient_patch = patchimg2differentview(patient_patch_img).reshape((1,3*((2*half_patch_size)+1),(2*half_patch_size)+1,(2*half_patch_size)+1)).astype(np.float32)\n",
    "            patient_patch = patient_patch_img.reshape((1,(2*half_patch_size)+1,(2*half_patch_size)+1,(2*half_patch_size)+1)).astype(np.float32)\n",
    "            patient_patch -= m\n",
    "            patient_patch /= s\n",
    "            predicted_label = int(predict_fn(patient_patch)[0])\n",
    "            \n",
    "            final_label[z,x,y] = predicted_label\n",
    "\n",
    "final_label_sitk = sitk.GetImageFromArray(final_label)\n",
    "sitk.WriteImage(final_label_sitk, '../test_label.nrrd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
