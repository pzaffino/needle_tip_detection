{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import lasagne\n",
    "from copy import deepcopy\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import random\n",
    "from skimage import exposure\n",
    "from skimage.morphology import binary_closing\n",
    "\n",
    "import sklearn.cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_patches(mri, label, half_patch_size=5, negative_subsample_ratio=200):\n",
    "    \n",
    "    assert mri.shape == label.shape\n",
    "    \n",
    "    z_size, x_size, y_size = mri.shape\n",
    "    positive_patches = []\n",
    "    negative_patches = []\n",
    "    \n",
    "    if np.sum(label) <= 0.5:\n",
    "        return ([], [])\n",
    "\n",
    "    for z in xrange(half_patch_size, z_size-half_patch_size):\n",
    "        if label[z,:,:].sum() > 1000: continue # workaround to remove white slices into the label dataset\n",
    "        for x in xrange(half_patch_size, x_size-half_patch_size):\n",
    "            for y in xrange(half_patch_size, y_size-half_patch_size):\n",
    "                if label[z,x,y] == 1:\n",
    "                    positive_patches.append(mri[z-half_patch_size:z+half_patch_size+1,x-half_patch_size:x+half_patch_size+1,y-half_patch_size:y+half_patch_size+1])\n",
    "                elif label[z,x,y] == 0:\n",
    "                    negative_patches.append(mri[z-half_patch_size:z+half_patch_size+1,x-half_patch_size:x+half_patch_size+1,y-half_patch_size:y+half_patch_size+1])\n",
    "    \n",
    "    random.shuffle(negative_patches)\n",
    "    number_of_negative_cases = int(len(negative_patches) / float(negative_subsample_ratio))\n",
    "    selected_negative_patches = deepcopy(negative_patches[:number_of_negative_cases])\n",
    "    del(negative_patches)\n",
    "    \n",
    "    return (positive_patches, selected_negative_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def patchimg2differentview(patch):\n",
    "    \n",
    "    z_size, x_size, y_size = patch.shape\n",
    "    \n",
    "    X = np.zeros((z_size * 3, x_size, y_size), dtype=np.float32)\n",
    "    \n",
    "    counter = 0\n",
    "    for z in xrange(z_size):\n",
    "        X[counter,:,:] = patch[z,:,:]\n",
    "        counter += 1\n",
    "    for x in xrange(x_size):\n",
    "        X[counter,:,:] = patch[:,x,:]\n",
    "        counter += 1\n",
    "    for y in xrange(y_size):\n",
    "        X[counter,:,:] = patch[:,:,y]\n",
    "        counter += 1\n",
    "    \n",
    "    return X.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def patches2CNNformat(patches, label, half_patch_size=5):\n",
    "    \n",
    "    X = np.zeros((len(patches), (2 * half_patch_size)+1, (half_patch_size*2)+1, (half_patch_size*2)+1), dtype=np.float32)\n",
    "    y = np.zeros((len(patches)), dtype=np.int32) * -1\n",
    "    \n",
    "    for i, patch in enumerate(patches):\n",
    "        #X[i,:,:,:] = patchimg2differentview(patch)\n",
    "        X[i,:,:,:] = patch\n",
    "        y[i] = label\n",
    "    \n",
    "    assert -1 not in y\n",
    "    \n",
    "    return X.astype(np.float32), y.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_resized_img(img, data_type = sitk.sitkFloat32):\n",
    "    \n",
    "    size = img.GetSize()\n",
    "    ratio = [1.0/i for i in img.GetSpacing()]\n",
    "    new_size = [int(size[i]/ratio[i]) for i in range(3)]\n",
    "    \n",
    "    rimage = sitk.Image(new_size, data_type)\n",
    "    rimage.SetSpacing((1,1,1))\n",
    "    rimage.SetOrigin(img.GetOrigin())\n",
    "    tx = sitk.Transform()\n",
    "    \n",
    "    interp = sitk.sitkLinear\n",
    "    if data_type == sitk.sitkInt16:\n",
    "        interp = sitk.sitkNearestNeighbor\n",
    "    \n",
    "    new_image = sitk.Resample(img, rimage, tx, interp, data_type)\n",
    "    \n",
    "    return sitk.GetArrayFromImage(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def needles2tips(only_needles, image_array, number_of_slices=3):\n",
    "    needles_masks_array = np.zeros_like(image_array).astype(float)  \n",
    "    for file_item in only_needles:\n",
    "        this_mask = file_item.astype(np.float)\n",
    "        if np.sum(this_mask) < (np.shape(this_mask)[0] * np.shape(this_mask)[1] * np.shape(this_mask)[2]):\n",
    "            this_mask = binary_closing(this_mask,selem=np.ones((3,3,3)))\n",
    "            found=False\n",
    "            row = np.shape(this_mask)[0]-1\n",
    "            while found==False and row > 0: #< np.shape(this_mask)[0]-1 :\n",
    "                #print(row)\n",
    "                this_row = this_mask[row,:,:]\n",
    "                if np.sum(this_row) > 0:\n",
    "                    #print(row)\n",
    "                    found = True\n",
    "                    temp = np.add(needles_masks_array[row:row+1+number_of_slices,:,:],this_mask[row:row+1+number_of_slices,:,:])\n",
    "                    temp[temp!=0] = 1\n",
    "                    needles_masks_array[row:row+1+number_of_slices,:,:] = temp \n",
    "                row -= 1\n",
    "    return needles_masks_array.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def needles2tips_PAOLO(needles, mri, number_of_slices=3):\n",
    "    tips = np.zeros_like(mri).astype(np.int32)\n",
    "    #print(tips.shape)\n",
    "    for needle in needles:\n",
    "        needle = needle.astype(np.int32)\n",
    "        #print(\"MIN %f, MAX %f\" % (needle.min(), needle.max()))\n",
    "        if np.sum(needle) < (np.shape(needle)[0] * np.shape(needle)[1] * np.shape(needle)[2]):\n",
    "            #print(\"Valid needle\")\n",
    "            needle = binary_closing(needle, selem=np.ones((3,3,3)))\n",
    "            needle[needle!=0]=1\n",
    "            #print(\" after closing: MIN %f, MAX %f \" % (needle.min(), needle.max()))\n",
    "            for z in range(np.shape(mri)[0]-1, 0, -1):\n",
    "                if 200 > np.sum(needle[z,:,:]) > 0.5 and z-number_of_slices-1 >= 0:\n",
    "                    #print(\" valid slice %d\" % z)\n",
    "                    tmp = deepcopy(needle)\n",
    "                    tmp[:z-number_of_slices-1,:,:] = 0\n",
    "                    tips[tmp!=0] = 1\n",
    "                    del(tmp)\n",
    "                    break\n",
    "    \n",
    "    tips[tips!=0]=1\n",
    "        \n",
    "    return tips.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_needles(needles, mri):\n",
    "    all_needles = np.zeros_like(mri).astype(np.int32)\n",
    "    \n",
    "    for needle in needles:\n",
    "        needle = needle.astype(np.int32)\n",
    "        needle[needle!=0]=1\n",
    "        \n",
    "        if np.sum(needle) < (needle.shape[0] * needle.shape[1] * needle.shape[2])/3.0:\n",
    "            all_needles[needle!=0] = 1\n",
    "    \n",
    "    all_needles[all_needles!=0]=1\n",
    "        \n",
    "    return all_needles.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_volume(img, half_patch_size=5):\n",
    "    npad = ((half_patch_size,half_patch_size),(half_patch_size,half_patch_size),(half_patch_size,half_patch_size))\n",
    "    return np.lib.pad(img, npad, \"constant\", constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_for_CNN(general_folder, half_patch_size=5):\n",
    "    folders_cases = os.listdir(general_folder)\n",
    "    \n",
    "    X_pos = np.ones((1, (2 * half_patch_size)+1, (half_patch_size*2)+1, (half_patch_size*2)+1), dtype=np.float32) * -1.0\n",
    "    y_pos = np.ones((1), dtype=np.int32) * -1.0\n",
    "    \n",
    "    X_neg = np.ones((1, (2 * half_patch_size)+1, (half_patch_size*2)+1, (half_patch_size*2)+1), dtype=np.float32) * -1.0\n",
    "    y_neg = np.ones((1), dtype=np.int32) * -1.0\n",
    "    \n",
    "    for folder_case in folders_cases:\n",
    "        print(\"Patient #%s\" % (folder_case))\n",
    "        full_case_path = general_folder + os.sep + folder_case\n",
    "        \n",
    "        volumetric_files = os.listdir(full_case_path)\n",
    "        \n",
    "        assert \"case.nrrd\" in volumetric_files\n",
    "        del(volumetric_files[volumetric_files.index(\"case.nrrd\")])\n",
    "        print(\" %d needles file\" %  (len(volumetric_files)))\n",
    "\n",
    "        \n",
    "        mri_sitk = sitk.ReadImage(full_case_path + os.sep + \"case.nrrd\")\n",
    "        \n",
    "        needles = []\n",
    "        valid_needles = 0\n",
    "        for volumetric_file in volumetric_files:\n",
    "            if volumetric_file == \"labelmap.nrrd\":\n",
    "                pass\n",
    "            label_sitk = sitk.ReadImage(full_case_path + os.sep + volumetric_file)\n",
    "            label = sitk.GetArrayFromImage(label_sitk)\n",
    "            background = int(np.around(np.percentile(label, 75), decimals=0))\n",
    "            #print('Background %d' % background)\n",
    "            filtered_label = np.zeros_like(label, dtype=np.int32)\n",
    "            filtered_label[label!=background] = 1\n",
    "            #print(np.sum(filtered_label))\n",
    "            \n",
    "            #label[label!=0]=1\n",
    "            if np.sum(filtered_label)>0:\n",
    "                needles.append(filtered_label)\n",
    "                valid_needles += 1\n",
    "                \n",
    "        print(\" %d valid needles\" %  (valid_needles))\n",
    "        \n",
    "        tips = merge_needles(needles, sitk.GetArrayFromImage(mri_sitk))\n",
    "        #tips_sitk = sitk.GetImageFromArray(tips)\n",
    "        #tips_sitk.CopyInformation(mri_sitk)\n",
    "        #tips = get_resized_img(tips_sitk, sitk.sitkInt16)\n",
    "        tips = pad_volume(tips, half_patch_size=half_patch_size)\n",
    "        tips[tips!=0]=1\n",
    "        \n",
    "        for z in range(tips.shape[0]):\n",
    "            if np.sum(tips[z,:,:]) > 500:\n",
    "                tips[z,:,:] = np.zeros_like(tips[z,:,:], dtype=np.int32)\n",
    "        \n",
    "        tips[tips!=0]=1\n",
    "        tips = tips.astype(np.int32)\n",
    "        #assert np.sum(tips) > 0.5\n",
    "        \n",
    "        mri = sitk.GetArrayFromImage(mri_sitk).astype(np.float32)\n",
    "        mri = pad_volume(mri, half_patch_size=half_patch_size)\n",
    "        \n",
    "        # DEBUG\n",
    "        #data_tag = str(datetime.datetime.now()).replace(\" \", \"_\")\n",
    "        #mri_sitk_DEBUG = sitk.GetImageFromArray(mri)\n",
    "        #tips_sitk_DEBUG = sitk.GetImageFromArray(tips)\n",
    "        #tips_sitk_DEBUG.CopyInformation(mri_sitk_DEBUG)\n",
    "        #sitk.WriteImage(mri_sitk_DEBUG, \"../DEBUG/mri_%s.nrrd\" % data_tag)\n",
    "        #sitk.WriteImage(tips_sitk_DEBUG, \"../DEBUG/tips_%s.nrrd\" % data_tag)\n",
    "        \n",
    "        positive_patches, negative_patches = extract_patches(mri, tips, half_patch_size=half_patch_size)\n",
    "        if len(positive_patches) > 0:\n",
    "            print(\" %d positive patches and %d negative patches\" % (len(positive_patches), len(negative_patches)))\n",
    "            X_temp_pos, y_temp_pos = patches2CNNformat(positive_patches, 1, half_patch_size=half_patch_size)\n",
    "            X_temp_neg, y_temp_neg = patches2CNNformat(negative_patches, 0, half_patch_size=half_patch_size)\n",
    "            \n",
    "            X_pos = np.concatenate((X_pos, X_temp_pos), axis=0)\n",
    "            y_pos = np.concatenate((y_pos, y_temp_pos), axis=0)\n",
    "            X_neg = np.concatenate((X_neg, X_temp_neg), axis=0)\n",
    "            y_neg = np.concatenate((y_neg, y_temp_neg), axis=0)\n",
    "    \n",
    "    assert -1 in X_pos[0,:,:,:] and y_pos[0] == -1\n",
    "    X_pos = X_pos[1:,:,:,:].astype(np.float32)\n",
    "    y_pos = y_pos[1:].astype(np.int32)\n",
    "    \n",
    "    assert -1 in X_neg[0,:,:,:] and y_neg[0] == -1\n",
    "    X_neg = X_neg[1:,:,:,:].astype(np.float32)\n",
    "    y_neg = y_neg[1:].astype(np.int32)\n",
    "    \n",
    "    return X_pos, y_pos, X_neg, y_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_Xy_posneg(X_pos, y_pos, X_neg, y_neg, test_size1=0.20, test_size2=0.33):\n",
    "    \n",
    "    X_pos_train, X_pos_test, y_pos_train, y_pos_test = sklearn.cross_validation.train_test_split(X_pos, y_pos, test_size=test_size1)\n",
    "    X_pos_test, X_pos_val, y_pos_test, y_pos_val = sklearn.cross_validation.train_test_split(X_pos_test, y_pos_test, test_size=test_size2)\n",
    "\n",
    "    X_neg_train, X_neg_test, y_neg_train, y_neg_test = sklearn.cross_validation.train_test_split(X_neg, y_neg, test_size=test_size1)\n",
    "    X_neg_test, X_neg_val, y_neg_test, y_neg_val = sklearn.cross_validation.train_test_split(X_neg_test, y_neg_test, test_size=test_size2)\n",
    "\n",
    "    X_train = np.concatenate((X_neg_train, X_pos_train), axis=0).astype(np.float32)\n",
    "    y_train = np.concatenate((y_neg_train, y_pos_train), axis=0).astype(np.int32)\n",
    "\n",
    "    X_val = np.concatenate((X_neg_val, X_pos_val), axis=0).astype(np.float32)\n",
    "    y_val = np.concatenate((y_neg_val, y_pos_val), axis=0).astype(np.int32)\n",
    "\n",
    "    X_test = np.concatenate((X_neg_test, X_pos_test), axis=0).astype(np.float32)\n",
    "    y_test = np.concatenate((y_neg_test, y_pos_test), axis=0).astype(np.int32)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "half_patch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_pos, y_pos, X_neg, y_neg = data_for_CNN(\"../LabelMaps\", half_patch_size=half_patch_size)\n",
    "#saved_data = np.load('../patches_jun27_halfpatchsize5odd.npz')\n",
    "#X, y = saved_data['arr_0'], saved_data['arr_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.savez('../patches_jul1_halfpatchsize5odd.npz', X_pos, y_pos, X_neg, y_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = join_Xy_posneg(X_pos, y_pos, X_neg, y_neg, test_size1=0.20, test_size2=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m, s = X_train.mean(), X_train.std()\n",
    "\n",
    "X_train -= m\n",
    "X_train /= s\n",
    "\n",
    "X_val -= m\n",
    "X_val /= s\n",
    "\n",
    "X_test -= m\n",
    "X_test /= s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('meanandstd_jan10.npz', m=m, s=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=True):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn(single_entry_shape, input_var=None):\n",
    "    \n",
    "    network = lasagne.layers.InputLayer(shape=(None, single_entry_shape[0], single_entry_shape[1], \n",
    "                                               single_entry_shape[2]),\n",
    "                                        input_var=input_var)\n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=480, filter_size=(3, 3),  #120\n",
    "            nonlinearity=lasagne.nonlinearities.leaky_rectify,\n",
    "            W=lasagne.init.HeNormal())\n",
    "    #network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=240, filter_size=(2, 2), #120\n",
    "            nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "    \n",
    "    network = lasagne.layers.DenseLayer(\n",
    "           lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=120, #120*2*2\n",
    "           nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "    \n",
    "    network = lasagne.layers.DenseLayer(\n",
    "           lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=60,\n",
    "           nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "    \n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=2,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batchsize = 64\n",
    "single_entry_shape = X_train.shape[1:]\n",
    "\n",
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "network = build_cnn(single_entry_shape, input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a loss expression for training, i.e., a scalar objective we want\n",
    "# to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var) # multiclass_hinge_loss\n",
    "loss = loss.mean()\n",
    "# We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "# Create update expressions for training, i.e., how to modify the\n",
    "# parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "# Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=0.001, momentum=0.9)\n",
    "\n",
    "# Create a loss expression for validation/testing. The crucial difference\n",
    "# here is that we do a deterministic forward pass through the network,\n",
    "# disabling dropout layers.\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction, target_var)\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                    dtype=theano.config.floatX)\n",
    "\n",
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    print(\"Epoch %d started on %s\" % (epoch + 1, now.strftime(\"%Y-%m-%d %H:%M\")))\n",
    "    \n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, batchsize, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batchsize, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "\n",
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, batchsize, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SAVE NETWORK\n",
    "np.savez('../model_needle_jan10.npz', *lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LOAD NETWORK\n",
    "\"\"\"\n",
    "with np.load('../model_needle_july1.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "lasagne.layers.set_all_param_values(network, param_values)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "predict_fn = theano.function([input_var], T.argmax(test_prediction, axis=1))\n",
    "predict_confidence = theano.function([input_var], test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind = 17\n",
    "a = X_test[ind,:,:,:].reshape((1,(2*half_patch_size)+1,(2*half_patch_size)+1,(2*half_patch_size)+1))\n",
    "predict_label_patch = predict_fn(a)[0]\n",
    "\n",
    "print(predict_label_patch, y_test[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imshow(a[0,5,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_pat_sitk = sitk.ReadImage('LabelMaps/64/case.nrrd')\n",
    "test_pat = sitk.GetArrayFromImage(sitk.ReadImage('../LabelMaps/64/case.nrrd')).astype(np.float32)\n",
    "test_pat = test_pat[140:,15:180,80:230]\n",
    "test_pat = pad_volume(test_pat, half_patch_size=half_patch_size)\n",
    "\n",
    "pat_sitk = sitk.GetImageFromArray(test_pat.astype(np.float32))\n",
    "sitk.WriteImage(pat_sitk, '../test_pat.nrrd')\n",
    "\n",
    "final_label = np.zeros_like(test_pat)\n",
    "test_pat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z_size, x_size, y_size = test_pat.shape\n",
    "\n",
    "for z in xrange(half_patch_size, z_size-half_patch_size):\n",
    "    print(z)\n",
    "    for x in xrange(half_patch_size, x_size-half_patch_size):\n",
    "        for y in xrange(half_patch_size, y_size-half_patch_size):\n",
    "            patient_patch_img = test_pat[z-half_patch_size:z+half_patch_size+1,x-half_patch_size:x+half_patch_size+1,y-half_patch_size:y+half_patch_size+1]\n",
    "            #patient_patch = patchimg2differentview(patient_patch_img).reshape((1,3*((2*half_patch_size)+1),(2*half_patch_size)+1,(2*half_patch_size)+1)).astype(np.float32)\n",
    "            patient_patch = patient_patch_img.reshape((1,(2*half_patch_size)+1,(2*half_patch_size)+1,(2*half_patch_size)+1)).astype(np.float32)\n",
    "            patient_patch -= m\n",
    "            patient_patch /= s\n",
    "            predicted_label = int(predict_fn(patient_patch)[0])\n",
    "            \n",
    "            final_label[z,x,y] = predicted_label\n",
    "\n",
    "final_label_sitk = sitk.GetImageFromArray(final_label)\n",
    "sitk.WriteImage(final_label_sitk, '../test_label.nrrd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
